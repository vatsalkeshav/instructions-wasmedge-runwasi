apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-api
  template:
    metadata:
      labels:
        app: llama-api
    spec:
      runtimeClassName: wasmedge  
      containers:
      - name: llama-api
        image: ghcr.io/second-state/llama-api-server:latest
        imagePullPolicy: Never 
        command: ["llama-api-server.wasm"]
        args:
        - "--prompt-template"
        - "$(PROMPT_TEMPLATE)"
        - "--ctx-size"
        - "$(CTX_SIZE)"
        - "--model-name"
        - "$(MODEL_NAME)"
        env:
        - name: WASMEDGE_PLUGIN_PATH
          value: "/home/$(whoami)/.wasmedge/plugin/libwasmedgePluginWasiNN.so"  # MIGHT NEED CHANGE
        
        - name: WASMEDGE_WASINN_PRELOAD
          value: "default:GGML:CPU:/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf"
        
        # reference the configmap
        - name: PROMPT_TEMPLATE
          valueFrom:
            configMapKeyRef:
              name: llama-config
              key: PROMPT_TEMPLATE
        - name: CTX_SIZE
          valueFrom:
            configMapKeyRef:
              name: llama-config
              key: CTX_SIZE
        - name: MODEL_NAME
          valueFrom:
            configMapKeyRef:
              name: llama-config
              key: MODEL_NAME
              
        volumeMounts:
        - name: models
          mountPath: /models  # where the model will appear in-container
      volumes:
      - name: models
        hostPath:
          path: /mnt/models  # where the model actually is on the host
          type: Directory