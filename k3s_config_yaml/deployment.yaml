apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-config
data:
  # WASMEDGE_PLUGIN_PATH: " " # maybe not needed
  PROMPT_TEMPLATE: "llama-3-chat"
  CTX_SIZE: "4096"
  MODEL_NAME: "llama-3-1b"
  MODEL_FILE: "Llama-3.2-1B-Instruct-Q5_K_M.gguf"
  MODEL_PATH: "/models"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: llama-api
  template:
    metadata:
      labels:
        app: llama-api
    spec:
      runtimeClassName: wasmedge
      containers:
        - name: llama-api
          image: ghcr.io/second-state/llama-api-server:latest
          imagePullPolicy: Never
          command: ["llama-api-server.wasm"]
          args:
            - "--prompt-template"
            - "$(PROMPT_TEMPLATE)"
            - "--ctx-size"
            - "$(CTX_SIZE)"
            - "--model-name"
            - "$(MODEL_NAME)"
          env:
            - name: WASMEDGE_PLUGIN_PATH
              value: "/usr/local/lib/wasmedge/plugin/libwasmedgePluginWasiNN.so" # MIGHT NEED CHANGE

            - name: WASMEDGE_WASINN_PRELOAD
              value: "default:GGML:CPU:/models/Llama-3.2-1B-Instruct-Q5_K_M.gguf"

            - name: LD_LIBRARY_PATH
              value: "/usr/local/lib/wasmedge/lib:/usr/local/lib/wasmedge:/usr/local/lib" # other stuff the plugin might need

            # reference the configmap
            - name: PROMPT_TEMPLATE
              valueFrom:
                configMapKeyRef:
                  name: llama-config
                  key: PROMPT_TEMPLATE
            # reference the configmap
            - name: CTX_SIZE
              valueFrom:
                configMapKeyRef:
                  name: llama-config
                  key: CTX_SIZE
            # reference the configmap
            - name: MODEL_NAME
              valueFrom:
                configMapKeyRef:
                  name: llama-config
                  key: MODEL_NAME
            # reference the configmap
            - name: MODEL_FILE
              valueFrom:
                configMapKeyRef:
                  name: llama-config
                  key: MODEL_FILE
            # reference the configmap
            - name: MODEL_PATH
              valueFrom:
                configMapKeyRef:
                  name: llama-config
                  key: MODEL_PATH

          volumeMounts:
            - name: models
              mountPath: /models
            - name: wasmedge-plugins
              mountPath: /usr/lib/wasmedge
              readOnly: true
      volumes:
        - name: models
          hostPath:
            path: /mnt/models
            type: Directory
        - name: wasmedge-plugins
          hostPath:
            path: /
            type: Directory

---
apiVersion: v1
kind: Service
metadata:
  name: llama-service
spec:
  selector:
    app: llama-api
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
  type: ClusterIP

---
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: wasmedge
handler: wasmedge
